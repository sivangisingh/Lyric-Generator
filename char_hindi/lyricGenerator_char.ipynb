{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Name: Lyric Generator\n",
    "Description:\n",
    "Implementing a Deep Neural network using LSTMs to create a character based lyric generator\n",
    "\n",
    "Details:\n",
    "1. step size: 40\n",
    "2. batch size: 128\n",
    "3. Epochs: 100\n",
    "4. Songs: 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qh1Mk6st3YGX",
    "outputId": "3870d975-28a9-4ec8-d417-b99a60027489"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.models import model_from_json\n",
    "from keras.layers import Activation,LSTM,Dense,CuDNNLSTM, Flatten, Bidirectional, Dropout\n",
    "from keras.optimizers import Adam\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.callbacks import LambdaCallback, ModelCheckpoint, EarlyStopping\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import re\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "maxlen = 80 ##timesteps\n",
    "epochs = 20\n",
    "# MIN_WORD_FREQUENCY = 10\n",
    "song_count = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lyrics(path):\n",
    "    '''\n",
    "    Function to load lyrics of all the artists in the input path\n",
    "    '''\n",
    "    lyrics = \"\"\n",
    "    for fn in os.listdir(path):\n",
    "        with open(os.path.join(path, fn), 'r') as song:\n",
    "            song_lyrics = clean_string(song.read())\n",
    "            lyrics += song_lyrics\n",
    "    return lyrics\n",
    "\n",
    "def clean_string(string):\n",
    "    \"\"\"\n",
    "    Cleans unwanted characters and words from string.\n",
    "    @param string: The string to be cleaned.\n",
    "    @return: The cleaned string.\n",
    "    \"\"\"\n",
    "    string = string.lower()  # lowercase\n",
    "\n",
    "    clean_words = []\n",
    "    for word in string.split():\n",
    "        # clean words with quotation marks on only one side\n",
    "        if word[0] == '\"' and word[-1] != '\"':\n",
    "            word = word[1:]\n",
    "        elif word[-1] == '\"' and word[0] != '\"':\n",
    "            word = word[-1]\n",
    "\n",
    "        # clean words with parenthases on only one side\n",
    "        if word[0] == '(' and word[-1] != ')':\n",
    "            word = word[1:]\n",
    "        elif word[-1] == ')' and word[0] != '(':\n",
    "            word = word[:-1]\n",
    "\n",
    "        clean_words.append(word)\n",
    "    return ' '.join(clean_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Input\n",
    "Parameters: 10 songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Reading the scraped Rap songs\n",
    "# text = load_lyrics(\"./RapLyrics-Scraper/my_lyrics_folder/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading the kaggle input ~55k songs\n",
    "df=pd.read_csv('../hindi_songs.csv')['text']\n",
    "data=np.array(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Reading the scraped pink floyed songs\n",
    "# df=pd.read_csv('./pink_floyd_lyrics.csv',header=0, error_bad_lines=False, delimiter='\\t')['text']\n",
    "# df = df.fillna('')\n",
    "# data=np.array(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating corpus(all the characters in all the songs concatenated)\n",
    "1. Converting all the characters to lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9qzGPjvI3YGl"
   },
   "outputs": [],
   "source": [
    "corpus=''\n",
    "for ix in range(len(data)):\n",
    "    corpus+=data[ix]\n",
    "corpus = corpus.lower()\n",
    "corpus = clean_string(corpus)\n",
    "corpus = re.sub('[^\\t\\n\\r\\f\\va-zA-Z\\.\\' ]+', '', corpus)\n",
    "# corpus = text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Vocabulary and char, index mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rho43V-J3YGr"
   },
   "outputs": [],
   "source": [
    "vocab=list(set(corpus))\n",
    "char_ix={c:i for i,c in enumerate(vocab)}\n",
    "ix_char={i:c for i,c in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FDWiKD1T3YG3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "['u', 'n', 'd', 'q', \"'\", 'e', 'h', 'z', 'j', 'm', 'w', 'y', '.', ' ', 'p', 't', 'f', 'k', 'b', 'o', 'v', 'x', 'l', 'i', 'g', 'c', 's', 'r', 'a']\n"
     ]
    }
   ],
   "source": [
    "vocab_size=len(vocab) ##Dimensions of each char\n",
    "print(vocab_size)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210894"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X46FgeuG3YG6"
   },
   "outputs": [],
   "source": [
    "sentences=[]\n",
    "next_char=[]\n",
    "for i in range(len(corpus)-maxlen-1):\n",
    "    sentences.append(corpus[i:i+maxlen])\n",
    "    next_char.append(corpus[i+maxlen])\n",
    "split_count = int(0.8 * len(corpus))\n",
    "sentences_test = sentences[split_count:]\n",
    "next_char_test = next_char[split_count:]\n",
    "sentences = sentences[:split_count]\n",
    "next_char = next_char[:split_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(sentence_list, next_word_list, batch_size):\n",
    "    '''\n",
    "    Generator function to generate the input/output data using\n",
    "    generators concept(to avoid RAM overflow)\n",
    "    '''\n",
    "    index = 0\n",
    "    while True:\n",
    "        x = np.zeros((batch_size, maxlen, vocab_size), dtype=np.bool)\n",
    "        y = np.zeros((batch_size, vocab_size), dtype=np.bool)\n",
    "        for i in range(batch_size):\n",
    "            for t, w in enumerate(sentence_list[index]):\n",
    "                x[i, t, char_ix[w]] = 1\n",
    "            y[i, char_ix[next_word_list[index]]] = 1\n",
    "\n",
    "            index = index + 1\n",
    "            if index == len(sentence_list):\n",
    "                index = 0\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mGj6PbK-3YHP",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_model(timesteps, vocab_size, hidden_size, no_layers=1, dropout=0.2):\n",
    "    '''\n",
    "    Creating the model\n",
    "    '''\n",
    "    model = Sequential()\n",
    "    for i in range(no_layers):\n",
    "        model.add(Bidirectional(CuDNNLSTM(hidden_size, return_sequences=True),\n",
    "                                input_shape=(timesteps, vocab_size)))\n",
    "    model.add(Flatten())\n",
    "#     model.add(Bidirectional(CuDNNLSTM(128), input_shape=(timesteps,vocab_size)))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(vocab_size))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.summary()\n",
    "    model.compile(optimizer=Adam(lr=0.01), loss='categorical_crossentropy')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_3 (Bidirection (None, 80, 128)           48640     \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 10240)             0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 10240)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 29)                296989    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 29)                0         \n",
      "=================================================================\n",
      "Total params: 345,629\n",
      "Trainable params: 345,629\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(maxlen, vocab_size, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def on_epoch_end(epoch, logs):\n",
    "    '''\n",
    "    Callback function to write output to file after each epoch\n",
    "    '''\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    examples_file.write('\\n----- Generating text after Epoch: %d\\n' % epoch)\n",
    "\n",
    "    # Randomly pick a seed sequence\n",
    "    seed_index = np.random.randint(len(sentences+sentences_test))\n",
    "    seed = (sentences+sentences_test)[seed_index]\n",
    "\n",
    "    for diversity in [0.3, 0.4, 0.5, 0.6, 0.7]:\n",
    "        sentence = seed\n",
    "        examples_file.write('----- Diversity:' + str(diversity) + '\\n')\n",
    "        examples_file.write('----- Generating with seed:\\n\"' + ' '.join(sentence) + '\"\\n')\n",
    "        examples_file.write(' '.join(sentence))\n",
    "\n",
    "        for i in range(50):\n",
    "            x_pred = np.zeros((1, maxlen, vocab_size))\n",
    "            for t, word in enumerate(sentence):\n",
    "                x_pred[0, t,char_ix[word]] = 1\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char_pred = ix_char[next_index]\n",
    "\n",
    "            sentence = sentence[1:]\n",
    "#             print(sentence)\n",
    "            sentence += next_char_pred\n",
    "\n",
    "            examples_file.write(\" \"+next_char_pred)\n",
    "        examples_file.write('\\n')\n",
    "    examples_file.write('='*80 + '\\n')\n",
    "#     examples_file.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opening the output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_file = open(\"output_data.txt\", \"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1yMxsErg3YHU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/iwonttellyouthat/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "file_path = \"./checkpoints/LSTM_LYRICS-epoch{epoch:03d}-words%d-sequence%d-minfreq%d-loss{loss:.4f}-acc{acc:.4f}-val_loss{val_loss:.4f}-val_acc{val_acc:.4f}\" % (\n",
    "    len(vocab),\n",
    "    maxlen,\n",
    "    10\n",
    ")\n",
    "# checkpoint = ModelCheckpoint(file_path, monitor='val_acc', save_best_only=True)\n",
    "\n",
    "# checkpoint_path = \"cp.ckpt\"\n",
    "# cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, \n",
    "#                                                  save_weights_only=True,\n",
    "#                                                  verbose=1)\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "callbacks_list = [print_callback]\n",
    "history = model.fit_generator(generator(sentences, next_char, BATCH_SIZE),\n",
    "    steps_per_epoch=int(len(sentences)/BATCH_SIZE) + 1,\n",
    "    epochs=epochs,\n",
    "    validation_data=generator(sentences_test, next_char_test, BATCH_SIZE),\n",
    "    validation_steps=int(len(sentences_test)/BATCH_SIZE) + 1,\n",
    "    callbacks = callbacks_list\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closing the output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Train Loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots( nrows=1, ncols=1 )  # create figure & 1 axis\n",
    "plt.plot(history.history['loss'])\n",
    "fig.savefig('Train_loss.png')   # save the figure to file\n",
    "plt.close(fig)    # close the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Validation Loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots( nrows=1, ncols=1 )  # create figure & 1 axis\n",
    "plt.plot(history.history['val_loss'])\n",
    "fig.savefig('Val_loss.png')   # save the figure to file\n",
    "plt.close(fig)    # close the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "70L9BUE23YHY",
    "outputId": "cc040da6-1b15-487f-c9da-47404be44dcd"
   },
   "outputs": [],
   "source": [
    "model.save('keras_model_char.hdf5')\n",
    "# loaded_model = keras.models.load_model('keras_model_char.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loaded_model = keras.models.load_model('keras_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    '''\n",
    "    Function to sample a character from the a given\n",
    "    list of probs\n",
    "    '''\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly pick a seed sequence\n",
    "# seed_index = np.random.randint(len(sentences_test))\n",
    "# seed = sentences_test[seed_index]\n",
    "seed_index = -150\n",
    "seed = corpus[-seed_index:-seed_index+maxlen]\n",
    "print(\"----- Actual\")\n",
    "print(corpus[-seed_index:-seed_index+maxlen+100])\n",
    "print()\n",
    "\n",
    "for diversity in [0.3, 0.4, 0.5, 0.6, 0.7]:\n",
    "    sentence = seed\n",
    "    print('----- Diversity:' + str(diversity) + '\\n')\n",
    "    print('----- Generating with seed:\\n\"' + ''.join(sentence) + '\"\\n')\n",
    "    print(''.join(sentence))\n",
    "\n",
    "    for i in range(100):\n",
    "        x_pred = np.zeros((1, maxlen, vocab_size))\n",
    "        for t, word in enumerate(sentence):\n",
    "            x_pred[0, t,char_ix[word]] = 1\n",
    "\n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds, diversity)\n",
    "        next_char_pred = ix_char[next_index]\n",
    "\n",
    "        sentence = sentence[1:]\n",
    "#             print(sentence)\n",
    "        sentence += next_char_pred\n",
    "\n",
    "        print(\"\"+next_char_pred, end=\"\")\n",
    "    print('\\n')\n",
    "print('='*80 + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sssDDxa23YHc"
   },
   "outputs": [],
   "source": [
    "# txt = corpus\n",
    "# start_index = 230\n",
    "\n",
    "generated = ''\n",
    "actual = ''\n",
    "# sent=txt[start_index:start_index+maxlen]\n",
    "sent = sentences_test[0]\n",
    "generated += sent\n",
    "actual += sent\n",
    "print(\"Input - \",generated)\n",
    "gen = generated\n",
    "for i in range(100):\n",
    "    x_sample=generated[i:i+maxlen]\n",
    "    x = np.zeros((1,maxlen,vocab_size))\n",
    "    for j in range(maxlen):\n",
    "        x[0,j,char_ix[x_sample[j]]] = 1\n",
    "    probs = model.predict(x)\n",
    "    probs = np.reshape(probs,probs.shape[1])\n",
    "#     ix = np.argmax(probs)\n",
    "    ix=np.random.choice(range(vocab_size),p=probs.ravel())\n",
    "    generated += ix_char[ix]\n",
    "    actual += next_char_test[i]\n",
    "# for i in range(100):\n",
    "#     x_sample=gen[i:i+maxlen]\n",
    "#     x=np.zeros((1,maxlen,vocab_size))\n",
    "#     for j in range(maxlen):\n",
    "#         x[0,j,char_ix[x_sample[j]]]=1\n",
    "#     probs=loaded_model.predict(x)[0]\n",
    "#     ix = np.argmax(probs)\n",
    "# #     ix=np.random.choice(range(vocab_size),p=probs.ravel())\n",
    "#     gen+=ix_char[ix]\n",
    "# # print(\"--------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generated - \")\n",
    "print(generated)\n",
    "print()\n",
    "print(\"Actual -\")\n",
    "print(actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "lyric_generator_by_character.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
