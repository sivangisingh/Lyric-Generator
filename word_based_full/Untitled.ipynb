{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for model: LSTM_5_1_64\n",
      "Epoch 1/10\n",
      "480/480 [==============================] - 17s 36ms/step - loss: 4.9475 - val_loss: 4.6188\n",
      "Epoch 2/10\n",
      "480/480 [==============================] - 9s 18ms/step - loss: 3.7828 - val_loss: 4.7558\n",
      "Epoch 3/10\n",
      "480/480 [==============================] - 9s 18ms/step - loss: 2.7632 - val_loss: 5.0941\n",
      "Epoch 4/10\n",
      "480/480 [==============================] - 9s 18ms/step - loss: 2.0219 - val_loss: 5.5218\n",
      "Epoch 5/10\n",
      "480/480 [==============================] - 8s 16ms/step - loss: 1.5824 - val_loss: 5.8989\n",
      "Epoch 6/10\n",
      "480/480 [==============================] - 9s 18ms/step - loss: 1.3140 - val_loss: 6.2365\n",
      "Epoch 7/10\n",
      "480/480 [==============================] - 9s 18ms/step - loss: 1.1024 - val_loss: 6.6554\n",
      "Epoch 8/10\n",
      "480/480 [==============================] - 9s 18ms/step - loss: 0.9521 - val_loss: 6.9016\n",
      "Epoch 9/10\n",
      "480/480 [==============================] - 9s 18ms/step - loss: 0.8564 - val_loss: 7.1981\n",
      "Epoch 10/10\n",
      "480/480 [==============================] - 9s 18ms/step - loss: 0.7721 - val_loss: 7.3376\n",
      "Running for model: LSTM_5_1_128\n",
      "Epoch 1/10\n",
      "480/480 [==============================] - 16s 34ms/step - loss: 4.9416 - val_loss: 4.6078\n",
      "Epoch 2/10\n",
      "480/480 [==============================] - 9s 19ms/step - loss: 3.7910 - val_loss: 4.7322\n",
      "Epoch 3/10\n",
      "274/480 [================>.............] - ETA: 3s - loss: 2.8814"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-7a1a5132cff9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    269\u001b[0m                                             \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_words_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m                                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m                                             verbose=1)\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# # Project Name: Lyric Generator\n",
    "# Description:\n",
    "# Implementing a Deep Neural network using LSTMs to create a character based lyric generator\n",
    "\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.models import model_from_json\n",
    "from keras.layers import Activation, LSTM, Dense, CuDNNLSTM, Flatten, Bidirectional, Dropout, CuDNNGRU\n",
    "from keras.optimizers import Adam\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.callbacks import LambdaCallback, ModelCheckpoint, EarlyStopping\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "np.random.seed(10)\n",
    "\n",
    "\n",
    "def load_lyrics(path):\n",
    "    '''\n",
    "    Function to load lyrics of all the artists in the input path\n",
    "    '''\n",
    "    lyrics = \"\"\n",
    "    for fn in os.listdir(path):\n",
    "        with open(os.path.join(path, fn), 'r') as song:\n",
    "            song_lyrics = clean_string(song.read())\n",
    "            lyrics += song_lyrics\n",
    "    return lyrics\n",
    "\n",
    "\n",
    "def clean_string(string):\n",
    "    \"\"\"\n",
    "    Cleans unwanted characters and words from string.\n",
    "    @param string: The string to be cleaned.\n",
    "    @return: The cleaned string.\n",
    "    \"\"\"\n",
    "    string = string.lower()  # lowercase\n",
    "\n",
    "    clean_words = []\n",
    "    for word in string.split():\n",
    "        if word[0] == '\"' and word[-1] != '\"':\n",
    "            word = word[1:]\n",
    "        elif word[-1] == '\"' and word[0] != '\"':\n",
    "            word = word[-1]\n",
    "\n",
    "        # clean words with parenthases on only one side\n",
    "        if word[0] == '(' and word[-1] != ')':\n",
    "            word = word[1:]\n",
    "        elif word[-1] == ')' and word[0] != '(':\n",
    "            word = word[:-1]\n",
    "\n",
    "        clean_words.append(word)\n",
    "    return ' '.join(clean_words)\n",
    "\n",
    "\n",
    "def generator(sentence_list, next_word_list, batch_size):\n",
    "    '''\n",
    "    Generator function to generate the input/output data using\n",
    "    generators concept(to avoid RAM overflow)\n",
    "    '''\n",
    "    index = 0\n",
    "    while True:\n",
    "        x = np.zeros((batch_size, maxlen, vocab_size), dtype=np.bool)\n",
    "        y = np.zeros((batch_size, vocab_size), dtype=np.bool)\n",
    "        for i in range(batch_size):\n",
    "            for t, w in enumerate(sentence_list[index]):\n",
    "                x[i, t, word_ix[w]] = 1\n",
    "            y[i, word_ix[next_word_list[index]]] = 1\n",
    "\n",
    "            index = index + 1\n",
    "            if index == len(sentence_list):\n",
    "                index = 0\n",
    "        yield x, y\n",
    "\n",
    "\n",
    "def create_LSTM(timesteps, vocab_size, no_layers=2, dropout=0.2):\n",
    "    '''\n",
    "    Creating the model\n",
    "    '''\n",
    "    model = Sequential()\n",
    "    for i in range(no_layers):\n",
    "        model.add(Bidirectional(CuDNNLSTM(128, return_sequences=True),\n",
    "                                input_shape=(timesteps, vocab_size)))\n",
    "    model.add(Flatten())\n",
    "#     model.add(Bidirectional(CuDNNLSTM(128), input_shape=(timesteps,vocab_size)))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(vocab_size))\n",
    "    model.add(Activation('softmax'))\n",
    "#     model.summary()\n",
    "    model.compile(optimizer=Adam(lr=0.01), loss='categorical_crossentropy')\n",
    "    return model\n",
    "\n",
    "def create_GRU(timesteps, vocab_size, no_layers=2, dropout=0.2):\n",
    "    '''\n",
    "    Creating the model\n",
    "    '''\n",
    "    model = Sequential()\n",
    "    for i in range(no_layers):\n",
    "        model.add(Bidirectional(CuDNNGRU(128, return_sequences=True),\n",
    "                                input_shape=(timesteps, vocab_size)))\n",
    "    model.add(Flatten())\n",
    "#     model.add(Bidirectional(CuDNNLSTM(128), input_shape=(timesteps,vocab_size)))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(vocab_size))\n",
    "    model.add(Activation('softmax'))\n",
    "#     model.summary()\n",
    "    model.compile(optimizer=Adam(lr=0.01), loss='categorical_crossentropy')\n",
    "    return model\n",
    "\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    '''\n",
    "    helper function to sample an index from a probability array\n",
    "    '''\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\n",
    "def on_epoch_end(epoch, logs):\n",
    "    '''\n",
    "    Callback function to write output to file after each epoch\n",
    "    '''\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    examples_file.write('\\n----- Generating text after Epoch: %d\\n' % epoch)\n",
    "\n",
    "    # Randomly pick a seed sequence\n",
    "    seed_index = np.random.randint(len(sentences+sentences_test))\n",
    "    seed = (sentences+sentences_test)[seed_index]\n",
    "    prev_word = None\n",
    "    for diversity in [0.3, 0.4, 0.5, 0.6, 0.7]:\n",
    "        sentence = seed\n",
    "        examples_file.write('----- Diversity:' + str(diversity) + '\\n')\n",
    "        examples_file.write(\n",
    "            '----- Generating with seed:\\n\"' + ' '.join(sentence) + '\"\\n')\n",
    "        examples_file.write(\"----- Generated lyrics:\\n\")\n",
    "        examples_file.write(' '.join(sentence))\n",
    "\n",
    "        for i in range(50):\n",
    "            x_pred = np.zeros((1, maxlen, vocab_size))\n",
    "            for t, word in enumerate(sentence):\n",
    "                x_pred[0, t, word_ix[word]] = 1\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_word_pred = ix_word[next_index]\n",
    "\n",
    "            sentence = sentence[1:]\n",
    "            sentence.append(next_word_pred)\n",
    "            if(prev_word == \"\\n\"):\n",
    "                examples_file.write(next_word_pred)\n",
    "            else:\n",
    "                examples_file.write(\" \" + next_word_pred)\n",
    "        examples_file.write('\\n\\n')\n",
    "    examples_file.write('='*80 + '\\n')\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "epochs = 10\n",
    "MIN_WORD_FREQUENCY = 10\n",
    "song_count = 1000\n",
    "\n",
    "\n",
    "# Reading the kaggle input ~55k songs\n",
    "df = pd.read_csv('../songdata.csv')['text'][:song_count]\n",
    "data = np.array(df)\n",
    "\n",
    "text = ''\n",
    "for ix in range(len(data)):\n",
    "    text += data[ix]\n",
    "# text = text.lower()\n",
    "text = text.lower().replace('\\n', ' \\n ')\n",
    "text = re.sub(\" +\", \" \", text)\n",
    "# print('Corpus length in characters:', len(text))\n",
    "corpus = [w for w in text.split(' ') if w.strip() != '' or w == '\\n'\n",
    "        and (w[0] not in [\"(\", \"[\"] and w[-1] not in [\")\", \"]\"])]\n",
    "while \"\" in corpus:\n",
    "    corpus.remove(\"\")\n",
    "# print('Corpus length in words:', len(corpus))\n",
    "\n",
    "\n",
    "word_freq = {}\n",
    "for word in corpus:\n",
    "    word_freq[word] = word_freq.get(word, 0) + 1\n",
    "\n",
    "ignored_words = set()\n",
    "for k, v in word_freq.items():\n",
    "    if word_freq[k] < MIN_WORD_FREQUENCY:\n",
    "        ignored_words.add(k)\n",
    "\n",
    "vocab = set(corpus)\n",
    "# print('Unique words before ignoring:', len(vocab))\n",
    "# print('Ignoring words with frequency <', MIN_WORD_FREQUENCY)\n",
    "vocab = sorted(set(vocab) - ignored_words)\n",
    "# print('Unique words after ignoring:', len(vocab))\n",
    "# print_vocabulary(vocabulary, words)\n",
    "\n",
    "\n",
    "# ### Creating Vocabulary and char, index mappings\n",
    "word_ix = {c: i for i, c in enumerate(vocab)}\n",
    "ix_word = {i: c for i, c in enumerate(vocab)}\n",
    "\n",
    "\n",
    "max_lens = [5, 10]\n",
    "models = [create_LSTM, create_GRU]\n",
    "names = [\"LSTM\", \"GRU\"]\n",
    "depths = [1,2]\n",
    "hidden_sizes = [64,128]\n",
    "\n",
    "train_errors = []\n",
    "val_errors = []\n",
    "plot_names = []\n",
    "\n",
    "for maxlen in max_lens:\n",
    "    for model_index, m1 in enumerate(models):\n",
    "        for depth in depths:\n",
    "            for hidden_size in hidden_sizes:\n",
    "\n",
    "                # ### Filtering corpus based on new vocabulary\n",
    "                out_name = names[model_index]\n",
    "                out_name = out_name + \"_\" + str(maxlen) + \"_\" + str(depth) + \"_\" + str(hidden_size)\n",
    "                print(\"Running for model: {0}\".format(out_name))\n",
    "                sentences = []\n",
    "                next_words = []\n",
    "                ignored = 0\n",
    "                for i in range(0, len(corpus) - maxlen):\n",
    "                    # Only add the sequences where no word is in ignored_words\n",
    "                    if len(set(corpus[i: i+maxlen+1]).intersection(ignored_words)) == 0:\n",
    "                        sentences.append(corpus[i: i + maxlen])\n",
    "                        next_words.append(corpus[i + maxlen])\n",
    "                    else:\n",
    "                        ignored = ignored + 1\n",
    "\n",
    "\n",
    "                # ### Creating the train and test datasets\n",
    "\n",
    "                split_count = int(0.8 * len(sentences))\n",
    "                sentences_test = sentences[split_count:]\n",
    "                next_words_test = next_words[split_count:]\n",
    "                sentences = sentences[:split_count]\n",
    "                next_words = next_words[:split_count]\n",
    "\n",
    "\n",
    "                # ### Check vocab size and corpus size\n",
    "\n",
    "                vocab_size = len(vocab)  # Dimentions of each char\n",
    "\n",
    "                model = m1(maxlen, vocab_size, no_layers=depth)\n",
    "\n",
    "                # ### Opening the output file\n",
    "                examples_file = open(\"./results/output_files/\" + out_name+\".txt\", \"w\")\n",
    "\n",
    "\n",
    "                # ### Training the model\n",
    "                print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "                early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "                callbacks_list = [print_callback]\n",
    "                history = model.fit_generator(generator(sentences, next_words, BATCH_SIZE),\n",
    "                                            steps_per_epoch=int(\n",
    "                                                len(sentences)/BATCH_SIZE) + 1,\n",
    "                                            epochs=epochs,\n",
    "                                            validation_data=generator(sentences_test, next_words_test, BATCH_SIZE), validation_steps=int(len(sentences_test)/BATCH_SIZE) + 1,\n",
    "                                            callbacks=callbacks_list,\n",
    "                                            verbose=0)\n",
    "\n",
    "\n",
    "                # ### Closing the output file\n",
    "\n",
    "                examples_file.close()\n",
    "\n",
    "                # ### Plotting Train Loss curve\n",
    "                train_errors.append(history.history['loss'])\n",
    "                val_errors.append(history.history['val_loss'])\n",
    "                plot_names.append(out_name)\n",
    "                # ### Saving the model to disk\n",
    "                model.save(\"./results/models/\" + out_name + \".hdf5\")\n",
    "\n",
    "for i in range(len(train_errors)):\n",
    "    plt.plot(train_errors[i], label=plot_names[i])\n",
    "plt.ylabel(\"Error\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05),\n",
    "          fancybox=True, shadow=True, ncol=5)\n",
    "plt.title(\"Train Loss vs Epochs\")\n",
    "plt.savefig('./results/figs/train_errors.png', bbox_inches='tight',dpi=1000)\n",
    "\n",
    "for i in range(len(val_errors)):\n",
    "    plt.plot(val_errors[i], label=plot_names[i])\n",
    "plt.ylabel(\"Error\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05),\n",
    "          fancybox=True, shadow=True, ncol=5)\n",
    "plt.title(\"Validation Loss vs Epochs\")\n",
    "plt.savefig('./results/figs/val_errors.png', bbox_inches='tight',dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
