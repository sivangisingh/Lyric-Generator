{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Name: Lyric Generator\n",
    "Description:\n",
    "Implementing a Deep Neural network using LSTMs to create a character based lyric generator\n",
    "\n",
    "Details:\n",
    "1. step size: 40\n",
    "2. batch size: 128\n",
    "3. Epochs: 100\n",
    "4. Songs: 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qh1Mk6st3YGX",
    "outputId": "3870d975-28a9-4ec8-d417-b99a60027489"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.models import model_from_json\n",
    "from keras.layers import Activation,LSTM,Dense,CuDNNLSTM, Flatten, Bidirectional, Dropout\n",
    "from keras.optimizers import Adam\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.callbacks import LambdaCallback, ModelCheckpoint, EarlyStopping\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "maxlen = 50 ##timesteps\n",
    "epochs = 10\n",
    "MIN_WORD_FREQUENCY = 10\n",
    "song_count = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lyrics(path):\n",
    "    '''\n",
    "    Function to load lyrics of all the artists in the input path\n",
    "    '''\n",
    "    lyrics = \"\"\n",
    "    for fn in os.listdir(path):\n",
    "        with open(os.path.join(path, fn), 'r') as song:\n",
    "            song_lyrics = clean_string(song.read())\n",
    "            lyrics += song_lyrics\n",
    "    return lyrics\n",
    "\n",
    "def clean_string(string):\n",
    "    \"\"\"\n",
    "    Cleans unwanted characters and words from string.\n",
    "    @param string: The string to be cleaned.\n",
    "    @return: The cleaned string.\n",
    "    \"\"\"\n",
    "    string = string.lower()  # lowercase\n",
    "\n",
    "    clean_words = []\n",
    "    for word in string.split():\n",
    "#         if(word and (word[0] == \"[\" and word[-1] == \"]\")\\\n",
    "#            or (word[0] == \"(\" and word[-1] == \")\")):\n",
    "#             continue\n",
    "        # clean words with quotation marks on only one side\n",
    "        if word[0] == '\"' and word[-1] != '\"':\n",
    "            word = word[1:]\n",
    "        elif word[-1] == '\"' and word[0] != '\"':\n",
    "            word = word[-1]\n",
    "\n",
    "        # clean words with parenthases on only one side\n",
    "        if word[0] == '(' and word[-1] != ')':\n",
    "            word = word[1:]\n",
    "        elif word[-1] == ')' and word[0] != '(':\n",
    "            word = word[:-1]\n",
    "\n",
    "        clean_words.append(word)\n",
    "    return ' '.join(clean_words)\n",
    "\n",
    "def clean_array(vec):\n",
    "    \"\"\"\n",
    "    Cleans unwanted characters and words from string.\n",
    "    @param string: The string to be cleaned.\n",
    "    @return: The cleaned string.\n",
    "    \"\"\"\n",
    "    clean_words = []\n",
    "    for word in vec:\n",
    "        if(word and (word[0] == \"[\" and word[-1] == \"]\")\\\n",
    "           or (word[0] == \"(\" and word[-1] == \")\")):\n",
    "            continue\n",
    "        # clean words with quotation marks on only one side\n",
    "        if word[0] == '\"' and word[-1] != '\"':\n",
    "            word = word[1:]\n",
    "        elif word[-1] == '\"' and word[0] != '\"':\n",
    "            word = word[-1]\n",
    "\n",
    "        # clean words with parenthases on only one side\n",
    "        if word[0] == '(' and word[-1] != ')':\n",
    "            word = word[1:]\n",
    "        elif word[-1] == ')' and word[0] != '(':\n",
    "            word = word[:-1]\n",
    "\n",
    "        clean_words.append(word)\n",
    "    return ' '.join(clean_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Input\n",
    "Parameters: 10 songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading the scraped Rap songs\n",
    "# text = load_lyrics(\"./RapLyrics-Scraper/my_lyrics_folder/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading the kaggle input ~55k songs\n",
    "df=pd.read_csv('../songdata.csv')['text'][:song_count]\n",
    "data=np.array(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading the scraped pink floyed songs\n",
    "# df=pd.read_csv('./pink_floyd_lyrics.csv',header=0, error_bad_lines=False, delimiter='\\t')['text']\n",
    "# df = df.fillna('')\n",
    "# data=np.array(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Look at her face, it's a wonderful face  \\nAnd it means something special to me  \\nLook at the way that she smiles when she sees me  \\nHow lucky can one fellow be?  \\n  \\nShe's just my kind of girl, she makes me feel fine  \\nWho could ever believe that she could be mine?  \\nShe's just my kind of girl, without her I'm blue  \\nAnd if she ever leaves me what could I do, what could I do?  \\n  \\nAnd when we go for a walk in the park  \\nAnd she holds me and squeezes my hand  \\nWe'll go on walking for hours and talking  \\nAbout all the things that we plan  \\n  \\nShe's just my kind of girl, she makes me feel fine  \\nWho could ever believe that she could be mine?  \\nShe's just my kind of girl, without her I'm blue  \\nAnd if she ever leaves me what could I do, what could I do?\\n\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'll never know why I had to go  \\nWhy I had to put up such a lousy rotten show  \\nBoy, I was tough, packing all my stuff  \\nSaying I don't need you anymore, I've had enough  \\nAnd now, look at me standing here again 'cause I found out that  \\nMa ma ma ma ma ma ma ma ma ma ma ma ma ma ma ma my life is here  \\nGotta have you near  \\n  \\nAs good as new, my love for you  \\nAnd keeping it that way is my intention  \\nAs good as new and growing too  \\nYes, I think it's taking on a new dimension  \\nIt's as good as new, my love for you  \\nJust like it used to be and even better  \\nAs good as new, thank God it's true  \\nDarling, we were always meant to stay together  \\n  \\nFeel like a creep, never felt so cheap  \\nNever had a notion that my love could be so deep  \\nHow could I make such a dumb mistake  \\nNow I know I'm not entitled to another break  \\nBut please, baby, I beg you to forgive 'cause I found out that  \\nMa ma ma ma ma ma ma ma ma ma ma ma ma ma ma ma my life is here  \\nGotta get you near  \\n  \\nI thought that our love was at an end but here I am again  \\n  \\nAs good as new, my love for you  \\nAnd keeping it that way is my intention  \\nAs good as new and growing too  \\nYes, I think it's taking on a new dimension  \\nIt's as good as new, my love for you  \\nJust like it used to be and even better  \\nAs good as new, thank God it's true  \\nDarling, we were always meant to stay together  \\n  \\nYes the love I have for you feels as good as new  \\nDarling, we were always meant to stay together\\n\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating corpus(all the characters in all the songs concatenated)\n",
    "1. Converting all the characters to lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9qzGPjvI3YGl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length in characters: 1167232\n",
      "Corpus length in words: 258606\n"
     ]
    }
   ],
   "source": [
    "text=''\n",
    "for ix in range(len(data)):\n",
    "    text+=data[ix]\n",
    "# text = text.lower()\n",
    "text = text.lower().replace('\\n', ' \\n ')\n",
    "text = re.sub(\" +\" , \" \", text)\n",
    "print('Corpus length in characters:', len(text))\n",
    "corpus = [w for w in text.split(' ') if w.strip() != '' or w == '\\n'\n",
    "          and (w[0] not in [\"(\",\"[\" ] and w[-1] not in [\")\",\"]\" ])]\n",
    "while \"\" in corpus:\n",
    "    corpus.remove(\"\")\n",
    "print('Corpus length in words:', len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"look at her face, it's a wonderful face \\n and it means something special to me \\n look at the way that she smiles when she sees me \\n how lucky can one fellow be? \\n \\n she's just my kind of girl, she makes me feel fine \\n who could ever believe that she could be mine? \\n she's just my kind of girl, without her i'm blue \\n and if she ever leaves me what could i do, what could i do? \\n \\n and when we go for a walk in the park \\n and she holds me and squeezes my hand \\n we'll go on walking for hours and talking \\n about all the things that we plan \\n \\n she's just my kind of girl, she makes me feel fine \\n who could ever believe that she could be mine? \\n she's just my kind of girl, without her i'm blue \\n and if she ever leaves me what could i do, what could i do? \\n \\n take it easy with me, please \\n touch me gently like a summer evening breeze \\n take your time, make it slow \\n andante, andante \\n just let the feeling grow \\n \\n make your fingers soft and light \\n let your body be the velvet of the night \\n tou\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering vocabulary based on word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = {}\n",
    "for word in corpus:\n",
    "    word_freq[word] = word_freq.get(word, 0) + 1\n",
    "\n",
    "ignored_words = set()\n",
    "for k, v in word_freq.items():\n",
    "    if word_freq[k] < MIN_WORD_FREQUENCY:\n",
    "        ignored_words.add(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words before ignoring: 12864\n",
      "Ignoring words with frequency < 10\n",
      "Unique words after ignoring: 1830\n"
     ]
    }
   ],
   "source": [
    "vocab = set(corpus)\n",
    "print('Unique words before ignoring:', len(vocab))\n",
    "print('Ignoring words with frequency <', MIN_WORD_FREQUENCY)\n",
    "vocab = sorted(set(vocab) - ignored_words)\n",
    "print('Unique words after ignoring:', len(vocab))\n",
    "# print_vocabulary(vocabulary, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Vocabulary and char, index mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rho43V-J3YGr"
   },
   "outputs": [],
   "source": [
    "word_ix={c:i for i,c in enumerate(vocab)}\n",
    "ix_word={i:c for i,c in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering corpus based on new vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored sequences: 239047\n",
      "Remaining sequences: 19509\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "next_words = []\n",
    "ignored = 0\n",
    "for i in range(0, len(corpus) - maxlen):\n",
    "    # Only add the sequences where no word is in ignored_words\n",
    "    if len(set(corpus[i: i+maxlen+1]).intersection(ignored_words)) == 0:\n",
    "        sentences.append(corpus[i: i + maxlen])\n",
    "        next_words.append(corpus[i + maxlen])\n",
    "    else:\n",
    "        ignored = ignored + 1\n",
    "print('Ignored sequences:', ignored)\n",
    "print('Remaining sequences:', len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X46FgeuG3YG6"
   },
   "outputs": [],
   "source": [
    "split_count = int(0.8 * len(sentences))\n",
    "sentences_test = sentences[split_count:]\n",
    "next_words_test = next_words[split_count:]\n",
    "sentences = sentences[:split_count]\n",
    "next_words = next_words[:split_count]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check vocab size and corpus size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FDWiKD1T3YG3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1830\n"
     ]
    }
   ],
   "source": [
    "vocab_size=len(vocab) ##Dimentions of each char\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "258606"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(sentence_list, next_word_list, batch_size):\n",
    "    '''\n",
    "    Generator function to generate the input/output data using\n",
    "    generators concept(to avoid RAM overflow)\n",
    "    '''\n",
    "    index = 0\n",
    "    while True:\n",
    "        x = np.zeros((batch_size, maxlen, vocab_size), dtype=np.bool)\n",
    "        y = np.zeros((batch_size, vocab_size), dtype=np.bool)\n",
    "        for i in range(batch_size):\n",
    "            for t, w in enumerate(sentence_list[index]):\n",
    "                x[i, t, word_ix[w]] = 1\n",
    "            y[i, word_ix[next_word_list[index]]] = 1\n",
    "\n",
    "            index = index + 1\n",
    "            if index == len(sentence_list):\n",
    "                index = 0\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mGj6PbK-3YHP",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_model(timesteps, vocab_size, no_layers=2,dropout=0.2):\n",
    "    '''\n",
    "    Creating the model\n",
    "    '''\n",
    "    model=Sequential()\n",
    "    for i in range(no_layers):\n",
    "        model.add(Bidirectional(CuDNNLSTM(128, return_sequences=True),input_shape=(timesteps,vocab_size)))\n",
    "    model.add(Flatten())\n",
    "#     model.add(Bidirectional(CuDNNLSTM(128), input_shape=(timesteps,vocab_size)))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(vocab_size))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.summary()\n",
    "    model.compile(optimizer=Adam(lr=0.01),loss='categorical_crossentropy')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/iwonttellyouthat/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/iwonttellyouthat/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_1 (Bidirection (None, 50, 256)           2007040   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 50, 256)           395264    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 12800)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12800)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1830)              23425830  \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1830)              0         \n",
      "=================================================================\n",
      "Total params: 25,828,134\n",
      "Trainable params: 25,828,134\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(maxlen, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    '''\n",
    "    helper function to sample an index from a probability array\n",
    "    '''\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def on_epoch_end(epoch, logs):\n",
    "    '''\n",
    "    Callback function to write output to file after each epoch\n",
    "    '''\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    examples_file.write('\\n----- Generating text after Epoch: %d\\n' % epoch)\n",
    "\n",
    "    # Randomly pick a seed sequence\n",
    "    seed_index = np.random.randint(len(sentences+sentences_test))\n",
    "    seed = (sentences+sentences_test)[seed_index]\n",
    "#     print(seed)\n",
    "\n",
    "    for diversity in [0.3, 0.4, 0.5, 0.6, 0.7]:\n",
    "        sentence = seed\n",
    "        examples_file.write('----- Diversity:' + str(diversity) + '\\n')\n",
    "        examples_file.write('----- Generating with seed:\\n\"' + ' '.join(sentence) + '\"\\n')\n",
    "        examples_file.write(\"----- Generated lyrics:\\n\")\n",
    "        examples_file.write(' '.join(sentence))\n",
    "\n",
    "        for i in range(50):\n",
    "            x_pred = np.zeros((1, maxlen, vocab_size))\n",
    "#             print(\"sentence len: {0}\".format(len(sentence)))\n",
    "            for t, word in enumerate(sentence):\n",
    "#                 print(word)\n",
    "                x_pred[0, t,word_ix[word]] = 1\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_word_pred = ix_word[next_index]\n",
    "\n",
    "            sentence = sentence[1:]\n",
    "#             print(sentence)\n",
    "            sentence.append(next_word_pred)\n",
    "\n",
    "            examples_file.write(\" \"+next_word_pred)\n",
    "        examples_file.write('\\n\\n')\n",
    "    examples_file.write('='*80 + '\\n')\n",
    "#     examples_file.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opening the output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_file = open(\"output_data_word.txt\", \"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1yMxsErg3YHU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/iwonttellyouthat/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "file_path = \"./checkpoints/LSTM_LYRICS-epoch{epoch:03d}-words%d-sequence%d-minfreq%d-loss{loss:.4f}-acc{acc:.4f}-val_loss{val_loss:.4f}-val_acc{val_acc:.4f}\" % (\n",
    "    len(vocab),\n",
    "    maxlen,\n",
    "    10\n",
    ")\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_acc', save_best_only=True)\n",
    "\n",
    "checkpoint_path = \"cp.ckpt\"\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, \n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "callbacks_list = [print_callback, cp_callback]\n",
    "history = model.fit_generator(generator(sentences, next_words, BATCH_SIZE),\n",
    "    steps_per_epoch=int(len(sentences)/BATCH_SIZE) + 1,\n",
    "    epochs=epochs,\n",
    "    validation_data=generator(sentences_test, next_words_test, BATCH_SIZE)\n",
    "                    ,validation_steps=int(len(sentences_test)/BATCH_SIZE) + 1,\n",
    "                   callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closing the output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Train Loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Validation Loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "70L9BUE23YHY",
    "outputId": "cc040da6-1b15-487f-c9da-47404be44dcd"
   },
   "outputs": [],
   "source": [
    "model.save('keras_model_word.hdf5')\n",
    "# loaded_model = keras.models.load_model('keras_model_word.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loaded_model = keras.models.load_model('keras_model.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_n(model, input_seq, len_out=10):\n",
    "    generated = []\n",
    "    actual = []\n",
    "    # sent=txt[start_index:start_index+maxlen]\n",
    "    sent = input_seq\n",
    "    generated += sent\n",
    "    gen = generated\n",
    "    for i in range(len_out):\n",
    "        x_sample=generated[i:i+maxlen]\n",
    "        x = np.zeros((1,maxlen,vocab_size))\n",
    "        for j in range(maxlen):\n",
    "            x[0,j,word_ix[x_sample[j]]] = 1\n",
    "        probs = model.predict(x)\n",
    "        probs = np.reshape(probs,probs.shape[1])\n",
    "        ix = np.argmax(probs)\n",
    "        ix=np.random.choice(range(vocab_size),p=probs.ravel())\n",
    "        generated.append(ix_word[ix])\n",
    "    return \" \".join(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = \"Is there anybody in there?\\nJust nod if you can hear me\\nIs there anyone at home?\"\n",
    "inp_seq = inp.lower().split(\" \")[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_n(model, inp_seq, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_test[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_words_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sssDDxa23YHc"
   },
   "outputs": [],
   "source": [
    "# txt = corpus\n",
    "# start_index = 230\n",
    "for j in range(0, 100, maxlen):\n",
    "    generated = []\n",
    "    actual = []\n",
    "    # sent=txt[start_index:start_index+maxlen]\n",
    "    sent = sentences_test[j]\n",
    "    generated += sent\n",
    "    actual += sent\n",
    "    print(\"#######################\")\n",
    "    print(\"Input - \",\" \".join(generated))\n",
    "    gen = generated\n",
    "    for i in range(min(100,len(generated))):\n",
    "        x_sample=generated[i:i+maxlen]\n",
    "        x = np.zeros((1,maxlen,vocab_size))\n",
    "        for k in range(maxlen):\n",
    "            x[0,k,word_ix[x_sample[k]]] = 1\n",
    "        probs = model.predict(x)\n",
    "        probs = np.reshape(probs,probs.shape[1])\n",
    "#         ix = np.argmax(probs)\n",
    "        ix=np.random.choice(range(vocab_size),p=probs.ravel())\n",
    "        generated.append(ix_word[ix])\n",
    "        actual.append(next_words_test[j+i])\n",
    "#         print(j)\n",
    "#         print(i)\n",
    "#         print(next_words_test[j+i])\n",
    "#         if(i==1):\n",
    "#             break\n",
    "    # for i in range(100):\n",
    "    #     x_sample=gen[i:i+maxlen]\n",
    "    #     x=np.zeros((1,maxlen,vocab_size))\n",
    "    #     for j in range(maxlen):\n",
    "    #         x[0,j,char_ix[x_sample[j]]]=1\n",
    "    #     probs=loaded_model.predict(x)[0]\n",
    "    #     ix = np.argmax(probs)\n",
    "    # #     ix=np.random.choice(range(vocab_size),p=probs.ravel())\n",
    "    #     gen+=ix_char[ix]\n",
    "    # # print(\"--------------\")\n",
    "    print(\"Actual ###############\")\n",
    "    print(\" \".join(actual))\n",
    "    print()\n",
    "    print(\"Generated ############### \")\n",
    "    print(\" \".join(generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generated ############### \")\n",
    "print(\" \".join(generated))\n",
    "print()\n",
    "print(\"Actual ###############\")\n",
    "print(\" \".join(actual))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. https://github.com/fpaupier/RapLyrics-Scraper/blob/master/lyrics_scraper.py\n",
    "2. https://towardsdatascience.com/ai-generates-taylor-swifts-song-lyrics-6fd92a03ef7e"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "lyric_generator_by_character.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
