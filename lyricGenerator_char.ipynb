{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Name: Lyric Generator\n",
    "Description:\n",
    "Implementing a Deep Neural network using LSTMs to create a character based lyric generator\n",
    "\n",
    "Details:\n",
    "1. step size: 40\n",
    "2. batch size: 128\n",
    "3. Epochs: 100\n",
    "4. Songs: 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qh1Mk6st3YGX",
    "outputId": "3870d975-28a9-4ec8-d417-b99a60027489"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.models import model_from_json\n",
    "from keras.layers import Activation,LSTM,Dense,CuDNNLSTM, Flatten, Bidirectional, Dropout\n",
    "from keras.optimizers import Adam\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.callbacks import LambdaCallback, ModelCheckpoint, EarlyStopping\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "maxlen = 40 ##timesteps\n",
    "epochs = 100\n",
    "# MIN_WORD_FREQUENCY = 10\n",
    "song_count = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lyrics(path):\n",
    "    '''\n",
    "    Function to load lyrics of all the artists in the input path\n",
    "    '''\n",
    "    lyrics = \"\"\n",
    "    for fn in os.listdir(path):\n",
    "        with open(os.path.join(path, fn), 'r') as song:\n",
    "            song_lyrics = clean_string(song.read())\n",
    "            lyrics += song_lyrics\n",
    "    return lyrics\n",
    "\n",
    "def clean_string(string):\n",
    "    \"\"\"\n",
    "    Cleans unwanted characters and words from string.\n",
    "    @param string: The string to be cleaned.\n",
    "    @return: The cleaned string.\n",
    "    \"\"\"\n",
    "    string = string.lower()  # lowercase\n",
    "\n",
    "    clean_words = []\n",
    "    for word in string.split():\n",
    "        # clean words with quotation marks on only one side\n",
    "        if word[0] == '\"' and word[-1] != '\"':\n",
    "            word = word[1:]\n",
    "        elif word[-1] == '\"' and word[0] != '\"':\n",
    "            word = word[-1]\n",
    "\n",
    "        # clean words with parenthases on only one side\n",
    "        if word[0] == '(' and word[-1] != ')':\n",
    "            word = word[1:]\n",
    "        elif word[-1] == ')' and word[0] != '(':\n",
    "            word = word[:-1]\n",
    "\n",
    "        clean_words.append(word)\n",
    "    return ' '.join(clean_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Input\n",
    "Parameters: 10 songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Reading the scraped Rap songs\n",
    "# text = load_lyrics(\"./RapLyrics-Scraper/my_lyrics_folder/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading the kaggle input ~55k songs\n",
    "df=pd.read_csv('./songdata.csv')['text'][:song_count]\n",
    "data=np.array(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Reading the scraped pink floyed songs\n",
    "# df=pd.read_csv('./pink_floyd_lyrics.csv',header=0, error_bad_lines=False, delimiter='\\t')['text']\n",
    "# df = df.fillna('')\n",
    "# data=np.array(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating corpus(all the characters in all the songs concatenated)\n",
    "1. Converting all the characters to lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9qzGPjvI3YGl"
   },
   "outputs": [],
   "source": [
    "corpus=''\n",
    "for ix in range(len(data)):\n",
    "    corpus+=data[ix]\n",
    "corpus = corpus.lower()\n",
    "corpus = clean_string(corpus)\n",
    "# corpus = text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Vocabulary and char, index mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rho43V-J3YGr"
   },
   "outputs": [],
   "source": [
    "vocab=list(set(corpus))\n",
    "char_ix={c:i for i,c in enumerate(vocab)}\n",
    "ix_char={i:c for i,c in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FDWiKD1T3YG3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n",
      "['1', '(', 'h', \"'\", 'p', '[', '5', 'c', '!', 'm', 'i', '7', '?', 'v', '2', ':', '8', 'u', 'k', 'z', 'x', 'e', 'a', 'y', '\"', 'b', ' ', '3', 'r', 'n', '.', '9', 'f', 'l', '6', 's', '0', ']', 'd', 'w', 't', ',', 'g', '-', 'q', 'j', '4', 'o', ')']\n"
     ]
    }
   ],
   "source": [
    "vocab_size=len(vocab) ##Dimensions of each char\n",
    "print(vocab_size)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1084724"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X46FgeuG3YG6"
   },
   "outputs": [],
   "source": [
    "sentences=[]\n",
    "next_char=[]\n",
    "for i in range(len(corpus)-maxlen-1):\n",
    "    sentences.append(corpus[i:i+maxlen])\n",
    "    next_char.append(corpus[i+maxlen])\n",
    "split_count = int(0.8 * len(corpus))\n",
    "sentences_test = sentences[split_count:]\n",
    "next_char_test = next_char[split_count:]\n",
    "sentences = sentences[:split_count]\n",
    "next_char = next_char[:split_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(sentence_list, next_word_list, batch_size):\n",
    "    '''\n",
    "    Generator function to generate the input/output data using\n",
    "    generators concept(to avoid RAM overflow)\n",
    "    '''\n",
    "    index = 0\n",
    "    while True:\n",
    "        x = np.zeros((batch_size, maxlen, vocab_size), dtype=np.bool)\n",
    "        y = np.zeros((batch_size, vocab_size), dtype=np.bool)\n",
    "        for i in range(batch_size):\n",
    "            for t, w in enumerate(sentence_list[index]):\n",
    "                x[i, t, char_ix[w]] = 1\n",
    "            y[i, char_ix[next_word_list[index]]] = 1\n",
    "\n",
    "            index = index + 1\n",
    "            if index == len(sentence_list):\n",
    "                index = 0\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mGj6PbK-3YHP",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_model(timesteps, vocab_size, no_layers=2):\n",
    "    '''\n",
    "    Creating the model\n",
    "    '''\n",
    "    model=Sequential()\n",
    "    model.add(CuDNNLSTM(128,input_shape=(timesteps,vocab_size)))\n",
    "#     for i in range(no_layers):\n",
    "#         model.add(Bidirectional(CuDNNLSTM(128,input_shape=(timesteps,vocab_size), return_sequences=True)))\n",
    "#     model.add(Flatten())\n",
    "    model.add(Dense(vocab_size))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.summary()\n",
    "    model.compile(optimizer=Adam(lr=0.01),loss='categorical_crossentropy')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cu_dnnlstm_4 (CuDNNLSTM)     (None, 128)               91648     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 49)                6321      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 49)                0         \n",
      "=================================================================\n",
      "Total params: 97,969\n",
      "Trainable params: 97,969\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(maxlen, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def on_epoch_end(epoch, logs):\n",
    "    '''\n",
    "    Callback function to write output to file after each epoch\n",
    "    '''\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    examples_file.write('\\n----- Generating text after Epoch: %d\\n' % epoch)\n",
    "\n",
    "    # Randomly pick a seed sequence\n",
    "    seed_index = np.random.randint(len(sentences+sentences_test))\n",
    "    seed = (sentences+sentences_test)[seed_index]\n",
    "\n",
    "    for diversity in [0.3, 0.4, 0.5, 0.6, 0.7]:\n",
    "        sentence = seed\n",
    "        examples_file.write('----- Diversity:' + str(diversity) + '\\n')\n",
    "        examples_file.write('----- Generating with seed:\\n\"' + ' '.join(sentence) + '\"\\n')\n",
    "        examples_file.write(' '.join(sentence))\n",
    "\n",
    "        for i in range(50):\n",
    "            x_pred = np.zeros((1, maxlen, vocab_size))\n",
    "            for t, word in enumerate(sentence):\n",
    "                x_pred[0, t,char_ix[word]] = 1\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char_pred = ix_char[next_index]\n",
    "\n",
    "            sentence = sentence[1:]\n",
    "#             print(sentence)\n",
    "            sentence += next_char_pred\n",
    "\n",
    "            examples_file.write(\" \"+next_char_pred)\n",
    "        examples_file.write('\\n')\n",
    "    examples_file.write('='*80 + '\\n')\n",
    "#     examples_file.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opening the output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_file = open(\"output_data.txt\", \"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1yMxsErg3YHU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/praveen/.local/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      " 262/6780 [>.............................] - ETA: 13:37 - loss: 2.4421"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-c39366effa48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_char_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                     \u001b[0;34m,\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                    callbacks = callbacks_list)\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0mbatch_index\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mdelta_t_median\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         if (self._delta_t_batch > 0. and\n\u001b[1;32m    119\u001b[0m            (delta_t_median > 0.95 * self._delta_t_batch and delta_t_median > 0.1)):\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mmedian\u001b[0;34m(a, axis, out, overwrite_input, keepdims)\u001b[0m\n\u001b[1;32m   3412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3414\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0marray_function_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_median_dispatcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3415\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3416\u001b[0m     \"\"\"\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "file_path = \"./checkpoints/LSTM_LYRICS-epoch{epoch:03d}-words%d-sequence%d-minfreq%d-loss{loss:.4f}-acc{acc:.4f}-val_loss{val_loss:.4f}-val_acc{val_acc:.4f}\" % (\n",
    "    len(vocab),\n",
    "    maxlen,\n",
    "    10\n",
    ")\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_acc', save_best_only=True)\n",
    "\n",
    "checkpoint_path = \"cp.ckpt\"\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, \n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=5)\n",
    "callbacks_list = [print_callback]\n",
    "history = model.fit_generator(generator(sentences, next_char, BATCH_SIZE),\n",
    "    steps_per_epoch=int(len(sentences)/BATCH_SIZE) + 1,\n",
    "    epochs=epochs,\n",
    "    validation_data=generator(sentences_test, next_char_test, BATCH_SIZE)\n",
    "                    ,validation_steps=int(len(sentences_test)/BATCH_SIZE) + 1,\n",
    "                   callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closing the output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Train Loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Validation Loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "70L9BUE23YHY",
    "outputId": "cc040da6-1b15-487f-c9da-47404be44dcd"
   },
   "outputs": [],
   "source": [
    "model.save('keras_model.hdf5')\n",
    "loaded_model = keras.models.load_model('keras_model.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loaded_model = keras.models.load_model('keras_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    '''\n",
    "    Function to sample a character from the a given\n",
    "    list of probs\n",
    "    '''\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sssDDxa23YHc"
   },
   "outputs": [],
   "source": [
    "# txt = corpus\n",
    "# start_index = 230\n",
    "\n",
    "generated = ''\n",
    "actual = ''\n",
    "# sent=txt[start_index:start_index+maxlen]\n",
    "sent = sentences_test[0]\n",
    "generated += sent\n",
    "actual += sent\n",
    "print(\"Input - \",generated)\n",
    "gen = generated\n",
    "for i in range(100):\n",
    "    x_sample=generated[i:i+maxlen]\n",
    "    x = np.zeros((1,maxlen,vocab_size))\n",
    "    for j in range(maxlen):\n",
    "        x[0,j,char_ix[x_sample[j]]] = 1\n",
    "    probs = model.predict(x)\n",
    "    probs = np.reshape(probs,probs.shape[1])\n",
    "    ix = np.argmax(probs)\n",
    "#     ix=np.random.choice(range(vocab_size),p=probs.ravel())\n",
    "    generated += ix_char[ix]\n",
    "    actual += next_char_test[i]\n",
    "# for i in range(100):\n",
    "#     x_sample=gen[i:i+maxlen]\n",
    "#     x=np.zeros((1,maxlen,vocab_size))\n",
    "#     for j in range(maxlen):\n",
    "#         x[0,j,char_ix[x_sample[j]]]=1\n",
    "#     probs=loaded_model.predict(x)[0]\n",
    "#     ix = np.argmax(probs)\n",
    "# #     ix=np.random.choice(range(vocab_size),p=probs.ravel())\n",
    "#     gen+=ix_char[ix]\n",
    "# # print(\"--------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generated - \")\n",
    "print(generated)\n",
    "print()\n",
    "print(\"Actual -\")\n",
    "print(actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "lyric_generator_by_character.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
